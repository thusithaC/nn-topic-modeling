{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-26 21:03:44.773 INFO transformers.file_utils:file_utils.py:39] PyTorch version 1.5.1 available.\n",
      "[2020-07-26 21:03:45.652 INFO transformers.file_utils:file_utils.py:55] TensorFlow version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from topic_model import TopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-26 21:03:48.613 INFO wealth-assistant-chat:mongo_helper.py:36] MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=False, maxpoolsize=100)\n",
      "[2020-07-26 21:03:48.613 INFO wealth-assistant-chat:mongo_helper.py:37] MongoDB successfully connected\n",
      "[2020-07-26 21:03:48.613 INFO wealth-assistant-chat:data_loader.py:61] loading data\n",
      "[2020-07-26 21:03:55.215 INFO wealth-assistant-chat:data_loader.py:63] loaded data 1361776 items\n",
      "[2020-07-26 21:04:00.603 INFO wealth-assistant-chat:data_loader.py:76] Vocab size 32512\n",
      "[2020-07-26 21:04:00.604 INFO wealth-assistant-chat:data_loader.py:80] creating label encoding\n",
      "[2020-07-26 21:04:00.620 INFO wealth-assistant-chat:data_loader.py:84] creating label encoding complete\n",
      "[2020-07-26 21:04:00.672 INFO wealth-assistant-chat:data_loader.py:90] encoding all data\n",
      "[2020-07-26 21:04:00.672 INFO root:SentenceTransformer.py:29] Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "[2020-07-26 21:04:00.673 INFO root:SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "[2020-07-26 21:04:00.673 INFO root:SentenceTransformer.py:67] Load SentenceTransformer from folder: /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n",
      "[2020-07-26 21:04:00.851 INFO transformers.configuration_utils:configuration_utils.py:262] loading configuration file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/config.json\n",
      "[2020-07-26 21:04:00.851 INFO transformers.configuration_utils:configuration_utils.py:300] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "[2020-07-26 21:04:00.852 INFO transformers.modeling_utils:modeling_utils.py:665] loading weights file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/pytorch_model.bin\n",
      "[2020-07-26 21:04:02.414 INFO transformers.modeling_utils:modeling_utils.py:765] All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "[2020-07-26 21:04:02.415 INFO transformers.modeling_utils:modeling_utils.py:774] All the weights of DistilBertModel were initialized from the model checkpoint at /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "[2020-07-26 21:04:02.415 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1167] Model name '/home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming '/home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[2020-07-26 21:04:02.416 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1197] Didn't find file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer.json. We won't load it.\n",
      "[2020-07-26 21:04:02.416 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1252] loading file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/vocab.txt\n",
      "[2020-07-26 21:04:02.416 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1252] loading file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/added_tokens.json\n",
      "[2020-07-26 21:04:02.417 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1252] loading file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/special_tokens_map.json\n",
      "[2020-07-26 21:04:02.417 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1252] loading file /home/thusitha/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer_config.json\n",
      "[2020-07-26 21:04:02.417 INFO transformers.tokenization_utils_base:tokenization_utils_base.py:1252] loading file None\n",
      "[2020-07-26 21:04:02.715 INFO root:SentenceTransformer.py:88] Use pytorch device: cuda\n",
      "[2020-07-26 21:04:03.905 INFO wealth-assistant-chat:data_loader.py:93] Initializing complete\n"
     ]
    }
   ],
   "source": [
    "topic_model = TopicModel(train_size=2000000, batch_size=50000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-26 21:04:13.500 INFO wealth-assistant-chat:topic_model.py:25] Starting Batch 1\n",
      "[2020-07-26 21:04:13.501 INFO wealth-assistant-chat:data_loader.py:102] Obtained next batch of tweets of length 50000\n",
      "[2020-07-26 21:04:13.502 INFO wealth-assistant-chat:data_loader.py:103] Creating BOW encoding\n",
      "100%|██████████| 6250/6250 [03:10<00:00, 32.81it/s]\n",
      "100%|██████████| 6250/6250 [03:15<00:00, 32.02it/s]\n",
      "100%|██████████| 6250/6250 [03:15<00:00, 32.02it/s]\n",
      "100%|██████████| 6250/6250 [03:15<00:00, 31.97it/s]\n",
      "100%|██████████| 6250/6250 [03:15<00:00, 31.93it/s]\n",
      "100%|██████████| 6250/6250 [03:16<00:00, 31.80it/s]\n",
      "100%|██████████| 6250/6250 [03:16<00:00, 31.77it/s] \n",
      "100%|██████████| 6250/6250 [03:16<00:00, 31.73it/s] \n",
      "/home/thusitha/work/projects/twitterSample/utils/data_loader.py:111: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.index_dd = np.array(encoded)\n",
      "[2020-07-26 21:07:35.056 INFO wealth-assistant-chat:data_loader.py:113] Creating BOW encoding complete\n",
      "[2020-07-26 21:07:35.057 INFO wealth-assistant-chat:data_loader.py:117] creating bert encodings\n",
      "[2020-07-26 21:08:29.207 INFO wealth-assistant-chat:data_loader.py:119] creating bert encodings complete\n",
      "[2020-07-26 21:11:22.965 INFO wealth-assistant-chat:topic_model.py:29] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:11:22.973 INFO wealth-assistant-chat:topic_model.py:30] [['just', 'today', '10', 'watching', 'video', 'old', 'wanted', 'to…', 'feeling', 'morning'], ['time', 'this', 'at', 'had', 'my', 'what', 'in', 'is', 'were', 'a'], ['u', '...', 'drop', 'ass', 'turn', 'fuck', 'rn', 'finna', 'im', 'handles'], ['ronald', 'biological', 'structure', 'dowoon', 'reagan', 'observing', 'unbearable', 'welcomed', 'refunded', '¿¿'], ['westandwitharnabforssr', 'bilal', 'nintendoswitch', 'spare', 'quite', 'megastarmammootty', '33yrsofgamechangernewdelhi', 'color', 'end', 'nyc'], ['dont', 'lol', 'fuck', 'knowing', 'shit', 'bad', 'hate', 'why', 'stupid', 'mean'], ['i', 'me', 'was', 'my', 'just', 'but', 'like', 'a', 'so', 'know'], ['an', 'new', 'at', 'be', 'album', 'tonight', 'my', 'taylor', 'll', 'folklore'], ['are', 'you', 'they', 'if', 'your', 'people', 'or', 'them', 'their', 'have'], ['__OOV__', 'for', 'and', 'our', 'to', 'on', 'in', 'we', 'more', 'from'], ['our', 'we', 'more', 'in', 'other', 'amp', 'are', 'from', 'country', 'world'], ['me', 'my', 'something', 'take', 'get', '”', '“', 'buy', 'm', 'yourself'], ['by', 'do', 'was', 'about', 'think', \"n't\", 'memories', 'but', 'how', 'that'], ['striker', 'trails', 'equality', 'desirable', 'ronald', 'dowoon', 'unbearable', 'innocence', 'hikes', 'comfort'], ['on', 'your', 'what', 'years', 'one', 'now', 'direction', '10yearsof1d', 'best', '1d'], ['every', 'miss', 'day', 'bless', 'innocence', 'blessed', 'desirable', 'lovely', 'smile', 'blessing'], ['portland', 'trump', 'mayor', 'president', 'federal', 'police', 'against', 'protesters', 'tear', 'wheeler'], ['happy', 'here', '10yearsofonedirection', 'relive', 'favourite', '10yearsof1d', 'memories', 'birthday', 'day', 'direction'], ['parts', 'ronald', 'fighters', 'desirable', 'describe', 'trails', 'relieve', 'unbearable', 'refunded', 'plastic'], ['convenient', 'desirable', 'horrible', 'trails', 'comprehensive', 'ronald', 'diaspora', 'equality', 'biological', 'friends/family'], ['s', 'with', 'it', 'us', '’', 'that', 'of', 'crew', 'person', 'o…'], ['’', 'i', 'for', 'that', 'been', 't', 'can', 'our', 'years', 'ten'], ['like', 'know', 'don', 'if', 'even', 'never', 'why', 'because', 't', 'say'], ['while', 't…', 'he', '20', 'over', 'bill', 'in', 'killed', 'income', 'floyd'], ['years', 'been', 'am', '10', 'today', 'last', 'how', 'over', 'into', 've'], ['me', 'm', 'so', 'love', 'want', 'hope', 'good', 'you', 'up', 'i'], ['follow', 'bts', 'amp', 'mtvhottest', '1', 'retweet', '__OOV__', 'giveaway', '3', '2'], ['instagram', 'renjun', 'cover', 'fools', 'teaser', 'blackpink', '2020', 'louis', 'harry', 'zayn'], ['s', 'with', 'it', 'us', '’', 'that', 'crew', 'of', 'person', 'o…'], ['who', 'are', 'people', 'all', 'your', 'these', 'us', 'incredible', 'amazing', 'fans'], ['what', 'when', 'we', 'no', 'time', 'at', 'had', 'were', 'would', 'there'], ['like', 't', 'if', 'they', 'know', 'don', 'because', 'why', 'she', 'say'], ['diaspora', 'unbearable', 'worl…', 'dowoon', 'convenient', 'revelation', 'ronald', 'refunded', 'arabia', 'desirable'], ['creating', 'ronald', 'equality', 'diaspora', 'unbearable', 'jesus', 'somewhat', 'beliefs', 'welcomed', 'structure'], ['he', 'his', 'a', 'him', 'they', 'as', '``', 'her', 'she', 'in'], ['unbearable', 'ronald', 'convenient', 'desirable', 'bn', 'relieve', 'abelism', 'diaspora', 'observing', 'tanking'], ['__OOV__', 'amp', 'openingday', '…', '•', 'gt', '3', 'style', 'king', 'free'], ['is', 'the', 'not', 'this', 'they', 'end', 'are', \"'s\", 'know', 'because'], ['trump', 'black', 'president', 'they', 'america', 'are', 'is', 'he', 'racist', 'doing'], ['to', 'a', 'and', 'they', 'in', '__OOV__', 'of', 'that', 'have', 'like'], ['now', '‘', 'music', 'video', 'on', 'spotify', 'h…', 'which', 'videos', 'million'], ['so', 'we', 'there', 'would', 'on', 'when', 'no', 'these', 'do', 'did'], ['study', 'chuckle', 'convenient', 'thread', 'strongly', 'grandchildren', 'arabia', 'armor', 'defined', 'desirable'], ['million', 'in', 'has', 'coronavirus', 'news', 'users', 'access', 'week', 'india', 'state'], ['will', 'have', '“', 'on', 'but', 'folklore', 'songs', 'the', 'deluxe', 'taylor'], ['his', '``', \"''\", 'man', 'him', 'amp', 'her', 'from', 'biden', 'a'], ['10yearsofonediretion', '10yearsofonedirection', 'zayn', 'waiting', 'harry', 'zouis', '😭', 'onedirection10years', 'clowning', 'ig'], ['with', 'o…', 'globe', 'flew', 'manager', 'mill', 'riggers', 'carpenters', 'person', 'stage'], ['you', 'and', 'be', 'so', 'to', 'love', 'we', 'thank', 'here', 'much'], ['our', 'be', 'will', 'thank', 'and', 'today', 'all', 'us', 'always', 'you']]\n",
      "[2020-07-26 21:11:22.974 INFO wealth-assistant-chat:topic_model.py:31] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:11:22.974 INFO wealth-assistant-chat:topic_model.py:25] Starting Batch 2\n",
      "[2020-07-26 21:11:22.978 INFO wealth-assistant-chat:data_loader.py:102] Obtained next batch of tweets of length 50000\n",
      "[2020-07-26 21:11:22.979 INFO wealth-assistant-chat:data_loader.py:103] Creating BOW encoding\n",
      "100%|██████████| 6250/6250 [03:09<00:00, 32.93it/s]\n",
      "100%|██████████| 6250/6250 [03:10<00:00, 32.80it/s]\n",
      "100%|██████████| 6250/6250 [03:12<00:00, 32.54it/s]\n",
      "100%|██████████| 6250/6250 [03:12<00:00, 32.43it/s]\n",
      "100%|██████████| 6250/6250 [03:13<00:00, 32.34it/s]\n",
      "100%|██████████| 6250/6250 [03:13<00:00, 32.30it/s]\n",
      "100%|██████████| 6250/6250 [03:13<00:00, 32.26it/s]\n",
      "100%|██████████| 6250/6250 [03:13<00:00, 32.27it/s]\n",
      "[2020-07-26 21:14:41.956 INFO wealth-assistant-chat:data_loader.py:113] Creating BOW encoding complete\n",
      "[2020-07-26 21:14:41.957 INFO wealth-assistant-chat:data_loader.py:117] creating bert encodings\n",
      "[2020-07-26 21:15:34.791 INFO wealth-assistant-chat:data_loader.py:119] creating bert encodings complete\n",
      "[2020-07-26 21:18:31.734 INFO wealth-assistant-chat:topic_model.py:29] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:18:31.743 INFO wealth-assistant-chat:topic_model.py:30] [['today', 'just', '10', 'old', 'morning', 'watching', 'feeling', 'to…', 'wanted', 'years'], ['what', 'time', 'at', 'when', 'ten', 'years', 'this', 'were', 'had', 'no'], ['...', 'more', 'may', 'please', 'help', 'home', 'relationship', '\\U0001f97a', 'cancer', 'virgo'], ['innocence', 'naval', 'observing', 'membership', 'unpleasant', 'candid', 'needless', 'charms', '더', 'disparities'], ['this', 'is', 'cooking', 'topic', 'literally', 'hilarious', 'quite', 'rice', '😂😂😂😂😂', 'dude'], ['same', 'shit', 'know', 'fuck', 'thinking', 'never', 'dont', 'bad', 'forget', 'lol'], ['i', 'my', 'me', 'just', 'was', 'it', 'so', 'like', 'but', 'm'], ['at', 'an', 'new', 'album', 'tonight', 'll', 'my', 'be', 'folklore', 'taylor'], ['you', 'your', 'if', 'are', 'or', 'they', 'their', 'have', 'someone', 'them'], ['on', '__OOV__', 'new', 'for', 'have', 'and', 'them', 'here', 'could', 'stop'], ['our', 'we', 'in', 'from', 'other', \"'ve\", 'are', 'find', 'work', 'deep'], ['my', 'me', 'take', 'get', 'something', 'm', 'come', 'buy', '”', \"'m\"], ['how', 'do', 'was', 'by', 'memories', 'about', 'but', 'think', \"n't\", 'incredible'], ['observing', 'naval', 'unto', 'unpleasant', 'needless', 'searched', 'charms', 'candid', 'uttered', 'membership'], ['on', 'one', 'now', 'best', 'direction', '1d', 'watch', 'what', 'moment', '10yearsof1d'], ['every', 'miss', 'day', 'grandma', 'good', 'laugh', 'everyday', 'left', 'crimes', 'blessed'], ['portland', 'mayor', 'federal', 'trump', 'trying', 'against', 'president', 'tear', 'ted', 'wheeler'], ['happy', 'here', 'memories', 'day', 'one', '10yearsof1d', 'relive', 'favourite', 'direction', 'your'], ['observing', 'naval', 'daughte…', 'steep', 'fling', 'unpleasant', 'desirable', 'needless', 'innocence', 'existing'], ['b/c', 'observing', 'daughte…', 'searched', 'naval', 'unto', 'converted', 'needless', 'taxed', '더'], ['s', 'it', 'with', 'us', 'crew', 'person', 'that', 'o…', '’', 'manager'], ['years', 'ten', 'been', '’', 'that', 'for', 'i', 'just', 't', 'can'], ['like', '``', \"''\", 'she', '“', 'her', '”', 'bitches', 'cute', 'does'], ['while', 't…', '20', 'bill', 'wearing', 'killed', 'he', 'in', 'special', 'mask'], ['ve', '’', 'everything', 'been', 'years', 'ten', 'how', 'over', 'am', 'last'], ['thank', 'so', 'you', 'all', 'up', 'those', 'good', 'who', 'love', 'whole'], ['follow', 'retweet', 'win', 'amp', 'mtvhottest', 'bts', '1', '2', 'giveaway', 'vote'], ['renjun', 'cover', 'photo', '200723', 'summer', 'instagram', '2020', 'fools', 'sehun', 'release'], ['world', 'history', 'has', 'person', 'barbara', 'president', 'united', 'bs', '23rd', 'first'], ['people', 'all', 'who', 'us', 'these', 'of', 'are', 'incredible', 'your', 'had'], ['we', 'when', 'so', 'no', 'on', 'what', 'do', 'would', 'there', 'memories'], ['if', 'like', 'know', 'can', 'don', 'you', 'do', 'they', 'me', \"n't\"], ['mtvhottest', 'paved', 'ssrdidntcommitsuicide', 'nfl', 'basketball', 'observing', 'champions', 'chapter', 'chelsea', 'westandwitharnabforssr'], ['innocence', 'candid', 'observing', 'intent', 'unpleasant', 'naval', 'needless', 'uttered', 'unhappy', 'disparities'], ['a', 'was', 'he', 'in', 'his', 'as', 'him', 'she', \"'s\", 'has'], ['innocence', 'naval', 'observing', 'fortunate', 'unpleasant', 'candid', 'daughte…', 'needless', 'uttered', 'shaun'], ['__OOV__', \"''\", '…', 'amp', '--', 'wind', 'ingredients', 'style', 'foodie', 'rain'], ['is', 'the', 'this', 'not', 'end', \"'s\", 'only', 'why', 'does', 'are'], ['trump', 'are', 'black', 'president', 'they', 'america', 'media', 'any', 'police', 'democrats'], ['to', 'a', 'and', 'of', 'be', 'that', 'it', '__OOV__', 'in', 'you'], ['on', 'now', 'video', '‘', 'music', 'stream', 'one', 'videos', 'including', 'spotify'], ['of', 'and', 'with', 'had', 'some', 'people', 'most', 'that', 've', 'i'], ['drugs', 'tax', 'study', 'drinking', 'costs', 'english', 'speaking', 'playstation', 'rm0.00', 'neck'], ['amp', 'million', 'users', 'access', 'made', 'yeah', 'in', 'unlimited', 'from', '🔥'], ['will', 'have', '“', 'folklore', 'taylor', 'songs', 'track', 'swift', 'but', 'edition'], ['his', 'he', '``', 'her', \"''\", 'man', 'a', 'him', \"'s\", 'she'], ['10yearsofonedirection', '10yearsofonediretion', 'clowning', 'kids', 'harry', 'zayn', 'waiting', 'directionersbreaktheinternet', 'asking', 'everyone'], ['they', 'time', 'their', 'only', 'are', 'because', 'now––w…', 'know', \"'s\", 'about'], ['be', 'so', 'to', 'everything', 't', 'we', 'and', 's', 'here', 'boys'], ['be', 'will', 'our', 'today', 'thank', 'just', 'always', 'us', 'all', 'it']]\n",
      "[2020-07-26 21:18:31.743 INFO wealth-assistant-chat:topic_model.py:31] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:18:31.743 INFO wealth-assistant-chat:topic_model.py:25] Starting Batch 3\n",
      "[2020-07-26 21:18:31.747 INFO wealth-assistant-chat:data_loader.py:102] Obtained next batch of tweets of length 50000\n",
      "[2020-07-26 21:18:31.747 INFO wealth-assistant-chat:data_loader.py:103] Creating BOW encoding\n",
      "100%|██████████| 6250/6250 [03:10<00:00, 32.77it/s]\n",
      "100%|██████████| 6250/6250 [03:19<00:00, 31.40it/s]\n",
      "100%|██████████| 6250/6250 [03:19<00:00, 31.34it/s]\n",
      "100%|██████████| 6250/6250 [03:19<00:00, 31.29it/s]\n",
      "100%|██████████| 6250/6250 [03:20<00:00, 31.22it/s]\n",
      "100%|██████████| 6250/6250 [03:20<00:00, 31.18it/s]\n",
      "100%|██████████| 6250/6250 [03:20<00:00, 31.15it/s]\n",
      "100%|██████████| 6250/6250 [03:21<00:00, 31.09it/s]\n",
      "[2020-07-26 21:21:57.599 INFO wealth-assistant-chat:data_loader.py:113] Creating BOW encoding complete\n",
      "[2020-07-26 21:21:57.600 INFO wealth-assistant-chat:data_loader.py:117] creating bert encodings\n",
      "[2020-07-26 21:22:52.595 INFO wealth-assistant-chat:data_loader.py:119] creating bert encodings complete\n",
      "[2020-07-26 21:25:57.308 INFO wealth-assistant-chat:topic_model.py:29] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:25:57.319 INFO wealth-assistant-chat:topic_model.py:30] [['just', 'today', '10', 'years', 'old', 'feeling', 'watching', 'morning', 'wanted', 'to…'], ['at', 'ten', 'years', 'when', 'what', 'time', 'no', 'this', 'for', 'were'], ['...', 'more', 'may', 'use', 'system', 'please', 'spaces', 'relationship', 'trail', 'pray'], ['noone', 'frontlines', 'willingly', 'dense', 'insanity', 'resisted', 'lump', 'transcript', 'quartz', 'dinosaurs'], ['this', 'is', 'not', 'shame', 'cooking', 'exposing', 'topic', 'hilarious', 'dude', 'rice'], ['same', 'thinking', 'shit', 'lol', 'girls', 'why', 'w', 'weight', 'funny', 'lmao'], ['i', 'my', 'just', 'me', 'was', 'so', 'm', 'out', 'like', 'but'], ['at', 'an', 'new', 'll', 'album', 'tonight', 'entire', 'releasing', 'folklore', 'my'], ['you', 'your', 'if', 'or', 'life', 'know', 'throat', 'someone', 'mi…', 're'], ['on', 'for', 'new', '__OOV__', 'have', 'here', 'and', 'them', 'please', 'from'], ['in', 'from', \"'ve\", 'of', 'our', 'find', 'deep', 'need', 'we', 'school'], ['my', 'me', 'something', 'take', 'm', 'come', 'get', 'buy', 'coworker', 'microbiologists'], ['do', 'how', 'memories', 'was', 'by', 'about', 'but', \"n't\", 'think', 'that'], ['noone', 'addressed', 'sand', 'resisted', 'frontlines', 'quartz', 'insanity', 'transcript', 'lahore', 'dense'], ['one', 'on', 'now', 'what', 'best', 'direction', '1d', 'watch', 'moment', 'years'], ['miss', 'every', 'good', 'signs', 'everyday', 'grandma', 'day', 'crimes', 'missed', 'war'], ['portland', 'mayor', 'federal', 'well', 'president', 'against', 'overcome', 'tear', 'night', 'bs'], ['here', 'happy', 'one', 'memories', 'day', '10yearsof1d', 'direction', 'your', 'favourite', 'relive'], ['noone', 'resisted', 'carefully', 'whilst', 'quartz', 'frontlines', 'transcript', 'innocence', 'dense', 'reproductive'], ['noone', 'responses', 'evenings', 'solely', 'resort', 'resisted', 'frontlines', 'b/c', 'thts', 'chosen'], ['s', 'it', 'with', 'us', 'that', 'person', 'crew', '’', 'o…', 'sound'], ['years', 'just', 'been', 'ten', 't', 'our', 'can', '’', 'that', 'for'], ['``', \"''\", 'she', '“', 'like', 'her', '”', 'girl', \"'s\", 'say'], ['t…', 'while', '20', 'bill', 'wearing', 'out…', 'killed', 'face', 'wear', 'mask'], ['’', 've', 'been', 'ten', 'years', 'how', 'everything', 'am', 'over', 'i'], ['thank', 'you', 'so', 'love', 'for', 'up', 'good', 'those', 'hope', 'proud'], ['follow', 'win', 'retweet', '1', 'let', 'amp', '2', 'next', 'keep', 'tweet'], ['renjun', 'sad', 'photo', '/…', 'cover', 'https', 'c…', 'summer', 'anna', 'sehun'], ['world', 'first', 'history', 'won', 'was', 'has', 'president', '23rd', 'remember', 'peace'], ['who', 'people', 'all', 'these', 'us', 'are', 'of', 'incredible', 'your', 'some'], ['we', 'when', 'no', 'do', 'so', 'on', 'there', 'memories', 'these', 'would'], ['can', 'if', 't', 'me', 'like', 'don', 'want', 'm', 'know', 'you'], ['paved', 'mtvhottest', 'mix', 'thread', 'bts', 'behind', 'shooting', 'yoongi', 'player', 'justin'], ['noone', 'frontlines', 'insanity', 'resisted', 'responses', 'transcript', 'lahore', 'quartz', 'dense', 'lacs'], ['a', 'he', 'in', 'his', 'was', 'has', 'as', \"'s\", 'him', 'that'], ['noone', 'frontlines', 'insanity', 'transcript', 'resisted', 'lahore', 'lump', 'quartz', 'regularly', 'dinosaurs'], ['__OOV__', \"''\", 'amp', '``', '…', 'bhi', 'ke', '보러', 'cotton', 'x'], ['the', 'is', 'this', 'not', \"'s\", 'end', 'world', 'it', 'see', 'only'], ['trump', 'are', 'black', 'america', 'president', 'they', 'white', 'any', 'biden', 'significant'], ['to', 'that', 'you', 'and', 'of', 'a', 'it', 'are', 'be', 'in'], ['on', '‘', 'one', 'stream', 'videos', 'now', 'spotify', 'live', 'twitter', 'including'], ['of', 'and', 'with', 'had', 've', 'i', 'some', 'most', 't', 'people'], ['focus', 'skill', 'drugs', 'ssrdidntcommitsuicide', 'speaking', 'blood', 'study', 'costs', 'drinking', 'focusing'], ['amp', 'million', 'nights', 'another', 'access', '🔥', 'users', 'rise', 'improvements', 'preparation'], ['will', '“', 'taylor', 'track', 'folklore', 'swift', '16', 'music', 'include', 'have'], ['his', 'he', 'him', 'her', 'man', 'said', 'she', 'surgery', 'picking', 'has'], ['10yearsofonedirection', '10yearsofonediretion', 'harry', 'everyone', 'directionersbreaktheinternet', 'also', 'asking', 'waiting', 'zayn', 'website'], ['they', 'time', 'are', 'their', 'that', 'going', 're', 'people', 'like', 'teens'], ['be', 'to', 'so', 's', 'boys', 't', 'everything', 'and', 'here', 'we'], ['all', 'today', 'will', 'our', 'us', 'and', 'be', 'always', 'boys', 'thank']]\n",
      "[2020-07-26 21:25:57.320 INFO wealth-assistant-chat:topic_model.py:31] \n",
      "---------------------------\n",
      "\n",
      "[2020-07-26 21:25:57.320 INFO wealth-assistant-chat:topic_model.py:25] Starting Batch 4\n",
      "[2020-07-26 21:25:57.323 INFO wealth-assistant-chat:data_loader.py:102] Obtained next batch of tweets of length 50000\n",
      "[2020-07-26 21:25:57.323 INFO wealth-assistant-chat:data_loader.py:103] Creating BOW encoding\n",
      "100%|██████████| 6250/6250 [03:15<00:00, 31.98it/s]\n",
      "100%|██████████| 6250/6250 [03:17<00:00, 31.67it/s]\n",
      "100%|██████████| 6250/6250 [03:18<00:00, 31.42it/s]\n",
      "100%|██████████| 6250/6250 [03:18<00:00, 31.43it/s]\n",
      "100%|██████████| 6250/6250 [03:19<00:00, 31.27it/s]\n",
      "100%|██████████| 6250/6250 [03:20<00:00, 31.18it/s]\n",
      "100%|██████████| 6250/6250 [03:20<00:00, 31.16it/s]\n",
      "100%|██████████| 6250/6250 [03:21<00:00, 31.01it/s] \n",
      "[2020-07-26 21:29:24.107 INFO wealth-assistant-chat:data_loader.py:113] Creating BOW encoding complete\n",
      "[2020-07-26 21:29:24.108 INFO wealth-assistant-chat:data_loader.py:117] creating bert encodings\n",
      "[2020-07-26 21:30:20.846 INFO wealth-assistant-chat:data_loader.py:119] creating bert encodings complete\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=391.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6726e742c8e4ca591ce093a0a665b53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100,)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/20]\tSamples: [50000/1000000]\tTrain Loss: 202.264352890625\tTime: 0:00:08.504233\n",
      "Epoch: [2/20]\tSamples: [100000/1000000]\tTrain Loss: 188.4086809375\tTime: 0:00:08.485038\n",
      "Epoch: [3/20]\tSamples: [150000/1000000]\tTrain Loss: 181.46252125\tTime: 0:00:08.469267\n",
      "Epoch: [4/20]\tSamples: [200000/1000000]\tTrain Loss: 176.530766875\tTime: 0:00:08.600395\n",
      "Epoch: [5/20]\tSamples: [250000/1000000]\tTrain Loss: 172.74687984375\tTime: 0:00:08.766759\n",
      "Epoch: [6/20]\tSamples: [300000/1000000]\tTrain Loss: 169.899643984375\tTime: 0:00:08.753572\n",
      "Epoch: [7/20]\tSamples: [350000/1000000]\tTrain Loss: 167.828625703125\tTime: 0:00:08.737533\n",
      "Epoch: [8/20]\tSamples: [400000/1000000]\tTrain Loss: 166.31779171875\tTime: 0:00:08.719166\n",
      "Epoch: [9/20]\tSamples: [450000/1000000]\tTrain Loss: 165.22236546875\tTime: 0:00:08.739362\n",
      "Epoch: [10/20]\tSamples: [500000/1000000]\tTrain Loss: 164.4123653125\tTime: 0:00:08.756032\n",
      "Epoch: [11/20]\tSamples: [550000/1000000]\tTrain Loss: 163.610694453125\tTime: 0:00:08.747860\n",
      "Epoch: [12/20]\tSamples: [600000/1000000]\tTrain Loss: 163.15540375\tTime: 0:00:08.737533\n",
      "Epoch: [13/20]\tSamples: [650000/1000000]\tTrain Loss: 162.71241953125\tTime: 0:00:08.722488\n",
      "Epoch: [14/20]\tSamples: [700000/1000000]\tTrain Loss: 162.396777421875\tTime: 0:00:08.710837\n",
      "Epoch: [15/20]\tSamples: [750000/1000000]\tTrain Loss: 161.998833828125\tTime: 0:00:08.709431\n",
      "Epoch: [16/20]\tSamples: [800000/1000000]\tTrain Loss: 161.76257375\tTime: 0:00:08.706791\n",
      "Epoch: [17/20]\tSamples: [850000/1000000]\tTrain Loss: 161.50763921875\tTime: 0:00:08.718290\n",
      "Epoch: [18/20]\tSamples: [900000/1000000]\tTrain Loss: 161.23004375\tTime: 0:00:08.704390\n",
      "Epoch: [19/20]\tSamples: [950000/1000000]\tTrain Loss: 161.05419828125\tTime: 0:00:08.727534\n",
      "Epoch: [20/20]\tSamples: [1000000/1000000]\tTrain Loss: 160.865299609375\tTime: 0:00:08.734014\n",
      "\n",
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100,)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/20]\tSamples: [50000/1000000]\tTrain Loss: 165.16453296875\tTime: 0:00:08.984286\n",
      "Epoch: [2/20]\tSamples: [100000/1000000]\tTrain Loss: 164.127493515625\tTime: 0:00:08.809197\n",
      "Epoch: [3/20]\tSamples: [150000/1000000]\tTrain Loss: 163.622805703125\tTime: 0:00:08.828308\n",
      "Epoch: [4/20]\tSamples: [200000/1000000]\tTrain Loss: 163.33863\tTime: 0:00:08.811261\n",
      "Epoch: [5/20]\tSamples: [250000/1000000]\tTrain Loss: 163.005250859375\tTime: 0:00:08.809461\n",
      "Epoch: [6/20]\tSamples: [300000/1000000]\tTrain Loss: 162.8506365625\tTime: 0:00:08.907172\n",
      "Epoch: [7/20]\tSamples: [350000/1000000]\tTrain Loss: 162.53102015625\tTime: 0:00:08.786475\n",
      "Epoch: [8/20]\tSamples: [400000/1000000]\tTrain Loss: 162.447283125\tTime: 0:00:08.831300\n",
      "Epoch: [9/20]\tSamples: [450000/1000000]\tTrain Loss: 162.4606940625\tTime: 0:00:08.881174\n",
      "Epoch: [10/20]\tSamples: [500000/1000000]\tTrain Loss: 162.25638171875\tTime: 0:00:08.802646\n",
      "Epoch: [11/20]\tSamples: [550000/1000000]\tTrain Loss: 162.141574609375\tTime: 0:00:08.853529\n",
      "Epoch: [12/20]\tSamples: [600000/1000000]\tTrain Loss: 161.91703328125\tTime: 0:00:08.825934\n",
      "Epoch: [13/20]\tSamples: [650000/1000000]\tTrain Loss: 161.76765875\tTime: 0:00:08.913516\n",
      "Epoch: [14/20]\tSamples: [700000/1000000]\tTrain Loss: 161.782578671875\tTime: 0:00:08.753558\n",
      "Epoch: [15/20]\tSamples: [750000/1000000]\tTrain Loss: 161.6359790625\tTime: 0:00:08.884257\n",
      "Epoch: [16/20]\tSamples: [800000/1000000]\tTrain Loss: 161.60565359375\tTime: 0:00:08.927234\n",
      "Epoch: [17/20]\tSamples: [850000/1000000]\tTrain Loss: 161.553268671875\tTime: 0:00:08.837525\n",
      "Epoch: [18/20]\tSamples: [900000/1000000]\tTrain Loss: 161.5005096875\tTime: 0:00:08.783667\n",
      "Epoch: [19/20]\tSamples: [950000/1000000]\tTrain Loss: 161.49540921875\tTime: 0:00:08.842417\n",
      "Epoch: [20/20]\tSamples: [1000000/1000000]\tTrain Loss: 161.24914546875\tTime: 0:00:08.844889\n",
      "\n",
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100,)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/20]\tSamples: [50000/1000000]\tTrain Loss: 161.47242578125\tTime: 0:00:09.213154\n",
      "Epoch: [2/20]\tSamples: [100000/1000000]\tTrain Loss: 161.01626078125\tTime: 0:00:09.274502\n",
      "Epoch: [3/20]\tSamples: [150000/1000000]\tTrain Loss: 160.8733359375\tTime: 0:00:09.360699\n",
      "Epoch: [4/20]\tSamples: [200000/1000000]\tTrain Loss: 160.706687890625\tTime: 0:00:09.260484\n",
      "Epoch: [5/20]\tSamples: [250000/1000000]\tTrain Loss: 160.56380546875\tTime: 0:00:09.240848\n",
      "Epoch: [6/20]\tSamples: [300000/1000000]\tTrain Loss: 160.42753109375\tTime: 0:00:09.296772\n",
      "Epoch: [7/20]\tSamples: [350000/1000000]\tTrain Loss: 160.358971328125\tTime: 0:00:09.197933\n",
      "Epoch: [8/20]\tSamples: [400000/1000000]\tTrain Loss: 160.27095296875\tTime: 0:00:09.157547\n",
      "Epoch: [9/20]\tSamples: [450000/1000000]\tTrain Loss: 160.28161640625\tTime: 0:00:09.299804\n",
      "Epoch: [10/20]\tSamples: [500000/1000000]\tTrain Loss: 160.2482384375\tTime: 0:00:09.129684\n",
      "Epoch: [11/20]\tSamples: [550000/1000000]\tTrain Loss: 160.14376109375\tTime: 0:00:09.141496\n",
      "Epoch: [12/20]\tSamples: [600000/1000000]\tTrain Loss: 160.08045265625\tTime: 0:00:09.294961\n",
      "Epoch: [13/20]\tSamples: [650000/1000000]\tTrain Loss: 160.10619921875\tTime: 0:00:09.215511\n",
      "Epoch: [14/20]\tSamples: [700000/1000000]\tTrain Loss: 160.04192140625\tTime: 0:00:09.181633\n",
      "Epoch: [15/20]\tSamples: [750000/1000000]\tTrain Loss: 160.008074453125\tTime: 0:00:09.267516\n",
      "Epoch: [16/20]\tSamples: [800000/1000000]\tTrain Loss: 159.9927071875\tTime: 0:00:09.231639\n",
      "Epoch: [17/20]\tSamples: [850000/1000000]\tTrain Loss: 159.861013828125\tTime: 0:00:09.184843\n",
      "Epoch: [18/20]\tSamples: [900000/1000000]\tTrain Loss: 159.895370859375\tTime: 0:00:09.257961\n",
      "Epoch: [19/20]\tSamples: [950000/1000000]\tTrain Loss: 159.907373125\tTime: 0:00:09.213190\n",
      "Epoch: [20/20]\tSamples: [1000000/1000000]\tTrain Loss: 159.97999046875\tTime: 0:00:09.245665\n",
      "\n",
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100,)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/20]\tSamples: [50000/1000000]\tTrain Loss: 159.6700034375\tTime: 0:00:09.344252\n",
      "Epoch: [2/20]\tSamples: [100000/1000000]\tTrain Loss: 159.4420340625\tTime: 0:00:09.523178\n",
      "Epoch: [3/20]\tSamples: [150000/1000000]\tTrain Loss: 159.09415453125\tTime: 0:00:09.131289\n",
      "Epoch: [4/20]\tSamples: [200000/1000000]\tTrain Loss: 158.99557484375\tTime: 0:00:09.556378\n",
      "Epoch: [5/20]\tSamples: [250000/1000000]\tTrain Loss: 158.941187578125\tTime: 0:00:09.508239\n",
      "Epoch: [6/20]\tSamples: [300000/1000000]\tTrain Loss: 158.81200625\tTime: 0:00:09.363563\n",
      "Epoch: [7/20]\tSamples: [350000/1000000]\tTrain Loss: 158.771904921875\tTime: 0:00:09.333336\n",
      "Epoch: [8/20]\tSamples: [400000/1000000]\tTrain Loss: 158.6890253125\tTime: 0:00:09.079113\n",
      "Epoch: [9/20]\tSamples: [450000/1000000]\tTrain Loss: 158.674088125\tTime: 0:00:08.946053\n",
      "Epoch: [10/20]\tSamples: [500000/1000000]\tTrain Loss: 158.6433625\tTime: 0:00:09.110355\n",
      "Epoch: [11/20]\tSamples: [550000/1000000]\tTrain Loss: 158.679773671875\tTime: 0:00:09.464966\n",
      "Epoch: [12/20]\tSamples: [600000/1000000]\tTrain Loss: 158.56145125\tTime: 0:00:09.008495\n",
      "Epoch: [13/20]\tSamples: [650000/1000000]\tTrain Loss: 158.42891640625\tTime: 0:00:09.223623\n",
      "Epoch: [14/20]\tSamples: [700000/1000000]\tTrain Loss: 158.4383121875\tTime: 0:00:08.954379\n",
      "Epoch: [15/20]\tSamples: [750000/1000000]\tTrain Loss: 158.44211109375\tTime: 0:00:09.516216\n",
      "Epoch: [16/20]\tSamples: [800000/1000000]\tTrain Loss: 158.3840940625\tTime: 0:00:09.381106\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=391.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e9becc519c14172929ac302caa2266f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=391.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f934628c1de9453d91a30e97cc4a33ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=391.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949abddd1961481ca6d237d1c2342963"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-cd973731286f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtopic_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/work/projects/twitterSample/topic_model.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoded_next_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0mtraining_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCTMDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbert_embeddings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0midx2token\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n---------------------------\\n\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_topic_lists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/work/virtual/twitterSample/lib/python3.6/site-packages/contextualized_topic_models/models/ctm.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_dataset, save_dir)\u001B[0m\n\u001B[1;32m    223\u001B[0m             \u001B[0;31m# train epoch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 225\u001B[0;31m             \u001B[0msp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_train_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    226\u001B[0m             \u001B[0msamples_processed\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0msp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0me\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/work/virtual/twitterSample/lib/python3.6/site-packages/contextualized_topic_models/models/ctm.py\u001B[0m in \u001B[0;36m_train_epoch\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0;31m# compute train loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m             \u001B[0msamples_processed\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 175\u001B[0;31m             \u001B[0mtrain_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m         \u001B[0mtrain_loss\u001B[0m \u001B[0;34m/=\u001B[0m \u001B[0msamples_processed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "topic_model.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[['we', 'time', 'this', 'for', 'our', 'is', 'pro', 'in', 'next', \"'s\"],\n ['trump',\n  'president',\n  'portland',\n  'federal',\n  'police',\n  'china',\n  'against',\n  'has',\n  'law',\n  'breaking'],\n ['harry',\n  'bts',\n  'mtvhottest',\n  'zayn',\n  'renjun',\n  'louis',\n  'replies',\n  'accepted',\n  'gain',\n  'ignore'],\n ['new',\n  'knee',\n  'folklore',\n  'tonight',\n  'music',\n  'before',\n  'taylor',\n  'year',\n  'anthem',\n  'school'],\n ['__OOV__', 'amp', '``', 'from', \"''\", 'by', 'of', 'via', '•', 'at'],\n ['carry',\n  'thoughts',\n  'step',\n  'dry',\n  'simply',\n  'deep',\n  'properly',\n  'sitting',\n  'baseball',\n  'speaking'],\n ['i', 'my', 'me', 'and', 'want', \"'m\", 'm', 'out', 'but', 'get'],\n ['would', 'there', 'but', 'we', 'no', 'was', 'thought', 'too', 'way', 'do'],\n ['so', 'all', 's', 'with', 'it', 'just', 'today', 'take', 'to', 'been'],\n ['direction',\n  'happy',\n  'old',\n  'watching',\n  '10',\n  'infection',\n  'one',\n  'missed',\n  'tough',\n  'morning'],\n ['you', 'your', 'if', 't', 'they', 'don', 're', 'are', 'normal', 'them'],\n ['beautiful',\n  'see',\n  'else',\n  'makes',\n  '...',\n  '👀',\n  'watch',\n  'come',\n  'ultra',\n  \"'what\"],\n ['camera',\n  'understands',\n  'hate',\n  'dude',\n  'comprehend',\n  'everywhere',\n  'real',\n  'tv',\n  'rice',\n  'lie'],\n ['’', 's', 'i', 't', 'over', 'm', 'into', 'how', 'am', 'last'],\n ['he', 'his', 'she', 'her', 'regarding', '“', 'a', 'deeply', '”', 'was'],\n ['believed',\n  'lo…',\n  'grasp',\n  'desire',\n  'b/c',\n  'males',\n  'basically',\n  'willing',\n  'gotten',\n  'driven'],\n ['2020',\n  '7',\n  '17',\n  'photo',\n  'dazed',\n  'update',\n  '⠀',\n  'release',\n  'points',\n  'v'],\n ['and', 'that', 'it', 'to', 'of', 'the', 'a', 'in', 'on', 'is'],\n ['yes',\n  'wow',\n  'ok',\n  'nice',\n  'sounds',\n  'cry',\n  'throwing',\n  'definitely',\n  'uh',\n  'cute'],\n ['us',\n  'around',\n  'up',\n  'world',\n  'such',\n  'for',\n  'an',\n  'everyone',\n  'or',\n  'take']]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.ctm.get_topic_lists(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}